{"cells":[{"cell_type":"markdown","metadata":{"id":"r6qZIX1MyEFd"},"source":["#**TO DO**\n","- train the god damn model\n","- try old school generator"]},{"cell_type":"markdown","metadata":{"id":"6zxV3XPJP321"},"source":["# **Download & Unzipping COCO dataset (USE ONLY FROM CPU TAB)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3tmpIcFEkgN"},"outputs":[],"source":["#Download Training Data\n","!wget http://images.cocodataset.org/zips/train2017.zip\n","!unzip /content/train2017.zip\n","!rm /content/train2017.zip\n","\n","#Download Validation Data\n","!wget http://images.cocodataset.org/zips/val2017.zip\n","!unzip /content/val2017.zip\n","!rm /content/val2017.zip\n","\n","#Download Testing Data\n","!wget http://images.cocodataset.org/zips/test2017.zip\n","!unzip /content/test2017.zip\n","!rm /content/test2017.zip\n","\n","#Download annotation for Training and Validation Data\n","!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","!unzip /content/annotations_trainval2017.zip\n","!rm /content/annotations_trainval2017.zip\n","\n","# delete the sameple folder data \n","%rm -rf /content/sample_data"]},{"cell_type":"markdown","metadata":{"id":"7DpkijkuQEct"},"source":["# ***Import Libraries***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xlj9rwejA_7V"},"outputs":[],"source":["import os\n","import cv2\n","import json\n","import string\n","import numpy as np\n","import pickle as pkl\n","from random import shuffle\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow as imshow\n","\n","import keras\n","import tensorflow as tf\n","from keras import Model\n","from keras.layers.merge import add\n","from keras.models import Sequential \n","from keras.utils.vis_utils import plot_model\n","from keras.utils import np_utils, data_utils\n","from tensorflow.keras.utils import to_categorical\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.image import ImageDataGenerator , load_img\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.layers.merge import Concatenate\n","from keras.layers import Dense, Conv2D, LSTM, Embedding ,Dropout, Activation, Input "]},{"cell_type":"markdown","source":["## **Downloading Data (USE ONLY FROM GPU TAB)**"],"metadata":{"id":"ZgfUhJJz2hD2"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#Download annotation for Training and Validation Data\n","!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","!unzip /content/annotations_trainval2017.zip\n","!rm /content/annotations_trainval2017.zip\n","\n","# copy resized images from drive\n","!cp -r \"/content/drive/MyDrive/Colab Notebooks/Project 2/Original/Content Folder/\" \"/content/\"\n","\n","!unzip \"/content/Content Folder/train2017.zip\" -d \"/\"\n","!unzip \"/content/Content Folder/val2017.zip\" -d \"/\"\n","!unzip \"/content/Content Folder/test2017.zip\" -d \"/\"\n","\n","%rm -rf \"/content/Content Folder\""],"metadata":{"id":"TsOH2MVv2L3M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v97W8GDlDYhU"},"source":["# **Creating Custom Tokenizer**\n","- **remove rare words**\n","- **replace rare word with \"\\<UNK>\" token**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYiGGetk4nsy"},"outputs":[],"source":["class Tokenizer():\n","  def __init__(self):\n","    self.VOCB_SIZE = 0\n","    \n","    self.deleted_words = list()\n","   \n","    self.word2int = dict()\n","    self.word2int['<UNK>'] = 0\n","    \n","    self.int2word = dict()\n"," \n","  def __len__(self):\n","    return self.VOCB_SIZE\n","\n","  def fit_on_texts(self,all_text):\n","    words_to_delete = list()\n","    count_dictionary = dict()\n","    \n","    #count how many each word appears\n","    for line in all_text:\n","      for word in line:\n","        if word not in count_dictionary:\n","          count_dictionary[word] = 0  \n","        \n","        count_dictionary[word]+=1\n","    \n","    #prepare words to be deleted of it appears less than 2 times\n","    for key,value in count_dictionary.items():\n","      if value <2:\n","        words_to_delete.append(key)\n","\n","    #delete rare words\n","    for word in words_to_delete:\n","      del count_dictionary[word]\n","    \n","    #create word to number dictionary\n","    i = 1\n","    for line in all_text:\n","      for word in line:\n","        if word in count_dictionary and word not in self.word2int:\n","          self.word2int[word] = i\n","          i+=1\n","\n","    \n","    #create number to word dictionary\n","    self.int2word = {num:word for word,num in self.word2int.items()}\n","\n","    self.VOCB_SIZE = len(list(self.word2int.items()))\n","    self.deleted_words = words_to_delete\n","\n","  def texts_to_sequences(self,text):\n","    seq = list()\n","    for word in text:\n","      if word in self.deleted_words:\n","        seq.append(0)\n","      else:\n","        seq.append(self.word2int[word])\n","    \n","    return seq\n","  \n","  def sequences_to_text(self,sequence):\n","    txt = [self.int2word(num) for num in sequence]\n","    return txt\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zx0AtxUcQNCV"},"source":["# **Prepare Data Paths**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPJ1bFFNgICP"},"outputs":[],"source":["origin_train_dir = '/content/train2017/'\n","origin_test_dir = '/content/test2017/'\n","origin_val_dir = '/content/val2017/'\n","caption_train_file_dir = '/content/annotations/captions_train2017.json'\n","caption_val_file_dir = '/content/annotations/captions_val2017.json'"]},{"cell_type":"markdown","metadata":{"id":"gH4wRiBkh7sD"},"source":["#**Resizing to 224 for ResNet50 (USE ONLY FROM CPU TAB)**\n","\n"," - **Resnet50 accept input of (224,224,3)**\n"," - **To free space I overwrite all images with its new size (224,224)**\n"," - **This will also speed up loading images instead of resizing every epoch**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucNIg5dEdVPy"},"outputs":[],"source":["def resize_to_224(img_dir):\n","  img = cv2.imread(img_dir)\n","  img = cv2.resize(img,(224,224))\n","  cv2.imwrite(img_dir,img)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCJj6bjpdWAq"},"outputs":[],"source":["for path in os.listdir(origin_train_dir):\n","  resize_to_224(origin_train_dir+path)\n","\n","\n","for path in os.listdir(origin_val_dir):\n","  resize_to_224(origin_val_dir+path)\n","\n","for path in os.listdir(origin_test_dir):\n","  resize_to_224(origin_test_dir+path)"]},{"cell_type":"markdown","source":["**zipping files to be passed to gpu**"],"metadata":{"id":"qIgLi6lB8Mcy"}},{"cell_type":"code","source":["!zip -r /content/val2017.zip /content/val2017\n","!zip -r /content/train2017.zip /content/train2017\n","!zip -r /content/test2017.zip /content/test2017"],"metadata":{"id":"UnslMSHghg3k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDDjpmeBTFXn"},"source":["#**Cleaning Caption**\n","- **Ronvert all words to lower case to avoid same seeing the word differently**\n","- **Remove any punctuation like (<, %, #, ... etc)**\n","- **Remove any word with 1 letter ( WE ARE LOOKING FOR WORDS !!!! بدهيات)**\n","- **Remove any numeric query keep just alphabetic queries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HPO5j8HeiXo"},"outputs":[],"source":["def clean_text(caption):\n","  table = str.maketrans(\" \",\" \",string.punctuation)\n","  caption = caption.split()\n","  \n","  caption = [word.lower() for word in caption]\n","  caption = [word.translate(table) for word in caption]\n","  caption = [word for word in caption if len(word)>1]  \n","  caption = [word for word in caption if word.isalpha()]\n","  \n","\n","  return caption"]},{"cell_type":"markdown","metadata":{"id":"v96BqiHqhcCa"},"source":["#**Loading images with corresponding caption in a Dictinary**\n","- **Keys are image paths**\n","- **Values are a list of corresponding captions for each Image**\n","- **it also return a list of all captions compined for future use**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYxwhrAVS3Mi"},"outputs":[],"source":["def extract_data (caption_dir):\n","  \n","  img_to_caption = dict()\n","  all_text = list()\n","\n","  with open(caption_dir, 'r') as f:\n","    jsonfile = json.load(f)\n","    annotations = jsonfile['annotations']\n","\n","  for annotation in annotations:\n","    img_id = annotation['image_id']\n","    caption = clean_text(annotation['caption'])\n","    \n","    #Add \"start\" at first of the caption\n","    caption.insert(0,\"<start> \")\n","    \n","    #and adding \"end\" at the end of the caption\n","    caption.append(\" <end>\")\n","    \n","    #remove any caption longer than 35 words + start and end words\n","    if len(caption) > 37:\n","      continue\n","\n","    all_text.append(caption)\n","\n","    img_id =  \"%012d\" % (img_id)   \n","\n","    if img_id not in img_to_caption:\n","      img_to_caption[img_id] = list()\n","    img_to_caption[img_id].append(caption)    \n","\n","  img_to_caption_list = [ [k,v] for k,v in img_to_caption.items()]\n","  return img_to_caption_list , all_text"]},{"cell_type":"markdown","metadata":{"id":"P7TTIQlUIfbO"},"source":["#**Extracting Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Te5IVqFGk3Tj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639485372999,"user_tz":-120,"elapsed":11781,"user":{"displayName":"FoRKsH Khadr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgGYrJmKW2D5d3Hldu9FCMJjHm6WXOV7S79GfIGBg=s64","userId":"14365323758798609600"}},"outputId":"db9bb3ec-bdcd-462a-90f6-6e1bd47d1b3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training elements 118287\n","Number of validation elements 5000\n"]}],"source":["train_list ,  train_text = extract_data(caption_train_file_dir)\n","val_list , val_text = extract_data(caption_val_file_dir)\n","\n","all_text = train_text + val_text\n","\n","size_of_training_data = len(train_list)\n","size_of_val_data = len(val_list)\n","\n","print(\"Number of training elements %d\" %size_of_training_data)\n","print(\"Number of validation elements %d\" %size_of_val_data)"]},{"cell_type":"markdown","metadata":{"id":"di5Cwf6l7iD7"},"source":["#**Unique words to tokens**\n","- **make a tokenizer of words where each word only appear once**\n","- **each word will have a number which is non repetative**\n","- **it will be used to generate numbers (model understand numbers only)  from every caption**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ro7AGKI8ysuP","executionInfo":{"status":"ok","timestamp":1639485661225,"user_tz":-120,"elapsed":3441,"user":{"displayName":"FoRKsH Khadr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgGYrJmKW2D5d3Hldu9FCMJjHm6WXOV7S79GfIGBg=s64","userId":"14365323758798609600"}},"outputId":"00c4da6e-e013-43ef-cc42-983ca5220b82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique words found 16759\n"]}],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(all_text)\n"," \n","VOCAB_SIZE = len(tokenizer)\n","\n","\n","print(\"Number of unique words found %d\" %VOCAB_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"4TOiddEL7pbL"},"source":["#**Find Longest Caption**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsUwW4mb19Y-","executionInfo":{"status":"ok","timestamp":1639485665184,"user_tz":-120,"elapsed":596,"user":{"displayName":"FoRKsH Khadr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgGYrJmKW2D5d3Hldu9FCMJjHm6WXOV7S79GfIGBg=s64","userId":"14365323758798609600"}},"outputId":"086f83a0-ef32-4aea-81a7-b91afaed0a88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of longest caption line 37\n"]}],"source":["def max_line_lenght(all_text):\n","  return max(len(line) for line in all_text )\n","\n","\n","MAX_CAPTION_LENGHT = max_line_lenght(all_text)\n","\n","\n","print(\"Size of longest caption line %d\" %MAX_CAPTION_LENGHT)"]},{"cell_type":"markdown","metadata":{"id":"9BU_YmKsuJGs"},"source":["#**Getting the Feature map Images with ResNet50**\n","- **Passing that image to ResNet and getting output Feature Map with** `extract_feature_map`\n","- **Saving feature Map to disk with** `save_feature_map`\n","- **getting Feature Map from disk with** `get_feature_map`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABN_R18dekSa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639485677301,"user_tz":-120,"elapsed":7190,"user":{"displayName":"FoRKsH Khadr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgGYrJmKW2D5d3Hldu9FCMJjHm6WXOV7S79GfIGBg=s64","userId":"14365323758798609600"}},"outputId":"85f5688b-638b-42c3-ae1a-42137a7714b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 1s 0us/step\n","94781440/94765736 [==============================] - 1s 0us/step\n"]}],"source":["EncoderNet = ResNet50(input_tensor=Input(shape=(224 , 224 , 3)),weights='imagenet',include_top=False,pooling='avg')\n","EncoderNet.trainable = False\n","\n","def extract_feature_map(image_path):\n","  #assert size is (224,224) avoid any error\n","  img = load_img(image_path,target_size=(224,224))\n","\n","  # add one dim at first (1,244,244,3) it will be used for batch size in future\n","  img = np.expand_dims(img, axis=0)\n","  img = EncoderNet.predict(img)\n","  img = np.reshape(img, img.shape[1])\n","  return img\n"]},{"cell_type":"markdown","source":["**Better use with GPU tab for faster process**"],"metadata":{"id":"9U0ltLqK_m5w"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0YXWbiKjE94","colab":{"base_uri":"https://localhost:8080/"},"outputId":"762534ee-aa77-4349-c57b-48bcf80fdbcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["1000 f_maps have passed\n","2000 f_maps have passed\n","3000 f_maps have passed\n","4000 f_maps have passed\n","5000 f_maps have passed\n","6000 f_maps have passed\n","7000 f_maps have passed\n","8000 f_maps have passed\n","9000 f_maps have passed\n","10000 f_maps have passed\n","11000 f_maps have passed\n","12000 f_maps have passed\n","13000 f_maps have passed\n","14000 f_maps have passed\n","15000 f_maps have passed\n","16000 f_maps have passed\n","17000 f_maps have passed\n","18000 f_maps have passed\n","19000 f_maps have passed\n","20000 f_maps have passed\n","21000 f_maps have passed\n","22000 f_maps have passed\n","23000 f_maps have passed\n","24000 f_maps have passed\n","25000 f_maps have passed\n","26000 f_maps have passed\n","27000 f_maps have passed\n","28000 f_maps have passed\n","29000 f_maps have passed\n","30000 f_maps have passed\n","31000 f_maps have passed\n","32000 f_maps have passed\n","33000 f_maps have passed\n","34000 f_maps have passed\n","35000 f_maps have passed\n","36000 f_maps have passed\n","37000 f_maps have passed\n","38000 f_maps have passed\n","39000 f_maps have passed\n","40000 f_maps have passed\n","41000 f_maps have passed\n","42000 f_maps have passed\n","43000 f_maps have passed\n","44000 f_maps have passed\n","45000 f_maps have passed\n","46000 f_maps have passed\n","47000 f_maps have passed\n","48000 f_maps have passed\n","49000 f_maps have passed\n","50000 f_maps have passed\n","51000 f_maps have passed\n","52000 f_maps have passed\n","53000 f_maps have passed\n","54000 f_maps have passed\n","55000 f_maps have passed\n","56000 f_maps have passed\n","57000 f_maps have passed\n","58000 f_maps have passed\n","59000 f_maps have passed\n","60000 f_maps have passed\n","61000 f_maps have passed\n","62000 f_maps have passed\n","63000 f_maps have passed\n","64000 f_maps have passed\n","65000 f_maps have passed\n","66000 f_maps have passed\n","67000 f_maps have passed\n","68000 f_maps have passed\n","69000 f_maps have passed\n","70000 f_maps have passed\n","71000 f_maps have passed\n","72000 f_maps have passed\n","73000 f_maps have passed\n","74000 f_maps have passed\n","75000 f_maps have passed\n","76000 f_maps have passed\n","77000 f_maps have passed\n","78000 f_maps have passed\n","79000 f_maps have passed\n","80000 f_maps have passed\n","81000 f_maps have passed\n","82000 f_maps have passed\n","83000 f_maps have passed\n","84000 f_maps have passed\n","85000 f_maps have passed\n","86000 f_maps have passed\n","87000 f_maps have passed\n","88000 f_maps have passed\n","89000 f_maps have passed\n","90000 f_maps have passed\n","91000 f_maps have passed\n","92000 f_maps have passed\n","93000 f_maps have passed\n","94000 f_maps have passed\n","95000 f_maps have passed\n","96000 f_maps have passed\n","97000 f_maps have passed\n","98000 f_maps have passed\n","99000 f_maps have passed\n","100000 f_maps have passed\n","101000 f_maps have passed\n","102000 f_maps have passed\n","103000 f_maps have passed\n","104000 f_maps have passed\n","105000 f_maps have passed\n","106000 f_maps have passed\n","107000 f_maps have passed\n","108000 f_maps have passed\n","109000 f_maps have passed\n","110000 f_maps have passed\n","111000 f_maps have passed\n","112000 f_maps have passed\n","113000 f_maps have passed\n","114000 f_maps have passed\n","115000 f_maps have passed\n","116000 f_maps have passed\n","117000 f_maps have passed\n","118000 f_maps have passed\n","119000 f_maps have passed\n","120000 f_maps have passed\n","121000 f_maps have passed\n","122000 f_maps have passed\n","123000 f_maps have passed\n"]}],"source":["def loading_to_memory(data_list,folder):\n","  for i , item in enumerate(data_list):\n","    dst_dir = folder + item[0] + '.jpg'\n","    img = extract_feature_map(dst_dir)\n","\n","    data_list[i][0] = img\n","  \n","  return data_list\n","\n","train_list = loading_to_memory(train_list,origin_train_dir)\n","val_list = loading_to_memory(val_list,origin_val_dir)"]},{"cell_type":"markdown","source":["# **Saving/Loading all important files to Magic file**\n","**This file contains:**\n","- training_list, val_list\n","- all_text_caption\n","- tokenizer\n","- VOCAB_SIZE\n","- MAX_CAPTION_LENGHT\n","- size_of_training_data\n","- size_of_val_data\n"],"metadata":{"id":"hJrKGZwz_rl4"}},{"cell_type":"markdown","source":["**SAVING**"],"metadata":{"id":"ESYZf0HzCnvp"}},{"cell_type":"code","source":["with open(\"/content/MagicFile.F\", \"wb\") as f :\n","  pkl.dump(train_list,f)\n","  pkl.dump(val_list,f)\n","  pkl.dump(all_text,f)\n","  pkl.dump(tokenizer,f)\n","  pkl.dump(VOCAB_SIZE,f)\n","  pkl.dump(MAX_CAPTION_LENGHT,f)\n","  pkl.dump(size_of_training_data,f)\n","  pkl.dump(size_of_val_data,f)"],"metadata":{"id":"BkiZBPCi_qhf"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["6zxV3XPJP321","7DpkijkuQEct","ZgfUhJJz2hD2","v97W8GDlDYhU","Zx0AtxUcQNCV","gH4wRiBkh7sD","oDDjpmeBTFXn","v96BqiHqhcCa","P7TTIQlUIfbO","di5Cwf6l7iD7","4TOiddEL7pbL"],"name":"Project 2 Prepare Data.ipynb","provenance":[],"mount_file_id":"13z3KnNcGvA7GKLmWQt5skg66ySMpDhYW","authorship_tag":"ABX9TyOyo0ZgX4SnBbpSgLik5+jy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}